{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Aneela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aneela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aneela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aneela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acq\n",
      "crude\n",
      "earn\n",
      "grain\n",
      "interest\n",
      "money-fx\n",
      "ship\n",
      "trade\n",
      "7691\n",
      "7691\n",
      "7691\n",
      "vectorize\n",
      "fit transform\n",
      "###############\n",
      "Kernal esmtp\n",
      "time 7:25:15.861550\n",
      "measure Accuracy\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9536395147313691 |\n",
      "| Split 2 | 0.9449740034662045 |\n",
      "| Split 3 | 0.9449740034662045 |\n",
      "| Split 4 | 0.9428076256499134 |\n",
      "| Split 5 | 0.9467071057192374 |\n",
      "| Average | 0.9466204506065858 |\n",
      "+---------+--------------------+\n",
      "Kernal esmtp\n",
      "time 7:25:15.861550\n",
      "measure Macro Precision\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9499223649968379 |\n",
      "| Split 2 | 0.9449919282164896 |\n",
      "| Split 3 | 0.9421159608478015 |\n",
      "| Split 4 | 0.9430899249184455 |\n",
      "| Split 5 | 0.9371241001968357 |\n",
      "| Average | 0.943448855835282  |\n",
      "+---------+--------------------+\n",
      "Kernal esmtp\n",
      "time 7:25:15.861550\n",
      "measure Micro Precision\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9536395147313691 |\n",
      "| Split 2 | 0.9449740034662045 |\n",
      "| Split 3 | 0.9449740034662045 |\n",
      "| Split 4 | 0.9428076256499134 |\n",
      "| Split 5 | 0.9467071057192374 |\n",
      "| Average | 0.9466204506065858 |\n",
      "+---------+--------------------+\n",
      "Kernal esmtp\n",
      "time 7:25:15.861550\n",
      "measure Macro Recall\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.8126396806334679 |\n",
      "| Split 2 | 0.7610115406557364 |\n",
      "| Split 3 | 0.7879712617590672 |\n",
      "| Split 4 | 0.7699535760285255 |\n",
      "| Split 5 | 0.7655384719156793 |\n",
      "| Average | 0.7794229061984952 |\n",
      "+---------+--------------------+\n",
      "Kernal esmtp\n",
      "time 7:25:15.861550\n",
      "measure Micro Recall\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9536395147313691 |\n",
      "| Split 2 | 0.9449740034662045 |\n",
      "| Split 3 | 0.9449740034662045 |\n",
      "| Split 4 | 0.9428076256499134 |\n",
      "| Split 5 | 0.9467071057192374 |\n",
      "| Average | 0.9466204506065858 |\n",
      "+---------+--------------------+\n",
      "Kernal esmtp\n",
      "time 7:25:15.861550\n",
      "measure F Measure\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.846403945709707  |\n",
      "| Split 2 | 0.8090943004426766 |\n",
      "| Split 3 | 0.8333927705870054 |\n",
      "| Split 4 | 0.8181498488935783 |\n",
      "| Split 5 | 0.8035617715439309 |\n",
      "| Average | 0.8221205274353798 |\n",
      "+---------+--------------------+\n",
      "Kernal esmtp\n",
      "time 7:25:15.861550\n",
      "measure Micro F Measure\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9536395147313691 |\n",
      "| Split 2 | 0.9449740034662045 |\n",
      "| Split 3 | 0.9449740034662045 |\n",
      "| Split 4 | 0.9428076256499134 |\n",
      "| Split 5 | 0.9467071057192374 |\n",
      "| Average | 0.9466204506065858 |\n",
      "+---------+--------------------+\n",
      "Kernal esmtp\n",
      "time 7:25:15.861550\n",
      "measure ROC\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9016650025268672 |\n",
      "| Split 2 | 0.8751222277516195 |\n",
      "| Split 3 | 0.8884592493537629 |\n",
      "| Split 4 | 0.8792239649366584 |\n",
      "| Split 5 | 0.8777926858168823 |\n",
      "| Average | 0.884452626077158  |\n",
      "+---------+--------------------+\n",
      "###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
      "<ipython-input-1-e73526fac075>:789: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernal EISC\n",
      "time 6:28:48.974531\n",
      "measure Accuracy\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9584055459272097 |\n",
      "| Split 2 | 0.9566724436741768 |\n",
      "| Split 3 | 0.9549393414211439 |\n",
      "| Split 4 | 0.9558058925476604 |\n",
      "| Split 5 | 0.9610051993067591 |\n",
      "| Average | 0.9573656845753901 |\n",
      "+---------+--------------------+\n",
      "Kernal EISC\n",
      "time 6:28:48.974531\n",
      "measure Macro Precision\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.8819784731826577 |\n",
      "| Split 2 | 0.9387840898016208 |\n",
      "| Split 3 | 0.879588775568749  |\n",
      "| Split 4 | 0.9036144536294473 |\n",
      "| Split 5 | 0.9299599629902682 |\n",
      "| Average | 0.9067851510345486 |\n",
      "+---------+--------------------+\n",
      "Kernal EISC\n",
      "time 6:28:48.974531\n",
      "measure Micro Precision\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9584055459272097 |\n",
      "| Split 2 | 0.9566724436741768 |\n",
      "| Split 3 | 0.9549393414211439 |\n",
      "| Split 4 | 0.9558058925476604 |\n",
      "| Split 5 | 0.9610051993067591 |\n",
      "| Average | 0.9573656845753901 |\n",
      "+---------+--------------------+\n",
      "Kernal EISC\n",
      "time 6:28:48.974531\n",
      "measure Macro Recall\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.8846585812131003 |\n",
      "| Split 2 | 0.8863967956855631 |\n",
      "| Split 3 | 0.8583917624527446 |\n",
      "| Split 4 | 0.8858071783017706 |\n",
      "| Split 5 | 0.877437339723099  |\n",
      "| Average | 0.8785383314752554 |\n",
      "+---------+--------------------+\n",
      "Kernal EISC\n",
      "time 6:28:48.974531\n",
      "measure Micro Recall\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9584055459272097 |\n",
      "| Split 2 | 0.9566724436741768 |\n",
      "| Split 3 | 0.9549393414211439 |\n",
      "| Split 4 | 0.9558058925476604 |\n",
      "| Split 5 | 0.9610051993067591 |\n",
      "| Average | 0.9573656845753901 |\n",
      "+---------+--------------------+\n",
      "Kernal EISC\n",
      "time 6:28:48.974531\n",
      "measure F Measure\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.8806755673799185 |\n",
      "| Split 2 | 0.909450735948002  |\n",
      "| Split 3 | 0.8658069689439883 |\n",
      "| Split 4 | 0.8932749283183681 |\n",
      "| Split 5 | 0.8999762411754393 |\n",
      "| Average | 0.8898368883531432 |\n",
      "+---------+--------------------+\n",
      "Kernal EISC\n",
      "time 6:28:48.974531\n",
      "measure Micro F Measure\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9584055459272097 |\n",
      "| Split 2 | 0.9566724436741768 |\n",
      "| Split 3 | 0.9549393414211439 |\n",
      "| Split 4 | 0.9558058925476604 |\n",
      "| Split 5 | 0.9610051993067591 |\n",
      "| Average | 0.9573656845753901 |\n",
      "+---------+--------------------+\n",
      "Kernal EISC\n",
      "time 6:28:48.974531\n",
      "measure ROC\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9386159014525862 |\n",
      "| Split 2 | 0.9391486231095934 |\n",
      "| Split 3 | 0.9249578450643949 |\n",
      "| Split 4 | 0.9385805593923997 |\n",
      "| Split 5 | 0.9352811194709848 |\n",
      "| Average | 0.9353168096979918 |\n",
      "+---------+--------------------+\n",
      "###############\n",
      "Kernal EDice\n",
      "time 4:02:49.223490\n",
      "measure Accuracy\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.957105719237435  |\n",
      "| Split 2 | 0.9501733102253033 |\n",
      "| Split 3 | 0.9480069324090121 |\n",
      "| Split 4 | 0.9467071057192374 |\n",
      "| Split 5 | 0.9506065857885615 |\n",
      "| Average | 0.9505199306759099 |\n",
      "+---------+--------------------+\n",
      "Kernal EDice\n",
      "time 4:02:49.223490\n",
      "measure Macro Precision\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9475555361929645 |\n",
      "| Split 2 | 0.9440444822665379 |\n",
      "| Split 3 | 0.9401955905978737 |\n",
      "| Split 4 | 0.9437461169626146 |\n",
      "| Split 5 | 0.9360064236050407 |\n",
      "| Average | 0.9423096299250062 |\n",
      "+---------+--------------------+\n",
      "Kernal EDice\n",
      "time 4:02:49.223490\n",
      "measure Micro Precision\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.957105719237435  |\n",
      "| Split 2 | 0.9501733102253033 |\n",
      "| Split 3 | 0.9480069324090121 |\n",
      "| Split 4 | 0.9467071057192374 |\n",
      "| Split 5 | 0.9506065857885615 |\n",
      "| Average | 0.9505199306759099 |\n",
      "+---------+--------------------+\n",
      "Kernal EDice\n",
      "time 4:02:49.223490\n",
      "measure Macro Recall\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.8412757831746998 |\n",
      "| Split 2 | 0.8068016154065669 |\n",
      "| Split 3 |  0.80074815889058  |\n",
      "| Split 4 | 0.7965867972843332 |\n",
      "| Split 5 | 0.7974898700164281 |\n",
      "| Average | 0.8085804449545216 |\n",
      "+---------+--------------------+\n",
      "Kernal EDice\n",
      "time 4:02:49.223490\n",
      "measure Micro Recall\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.957105719237435  |\n",
      "| Split 2 | 0.9501733102253033 |\n",
      "| Split 3 | 0.9480069324090121 |\n",
      "| Split 4 | 0.9467071057192374 |\n",
      "| Split 5 | 0.9506065857885615 |\n",
      "| Average | 0.9505199306759099 |\n",
      "+---------+--------------------+\n",
      "Kernal EDice\n",
      "time 4:02:49.223490\n",
      "measure F Measure\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.8769982733207993 |\n",
      "| Split 2 | 0.8540492393304171 |\n",
      "| Split 3 | 0.8403659525273441 |\n",
      "| Split 4 | 0.8409123668837513 |\n",
      "| Split 5 | 0.8412081772779247 |\n",
      "| Average | 0.8507068018680473 |\n",
      "+---------+--------------------+\n",
      "Kernal EDice\n",
      "time 4:02:49.223490\n",
      "measure Micro F Measure\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.957105719237435  |\n",
      "| Split 2 | 0.9501733102253033 |\n",
      "| Split 3 | 0.9480069324090121 |\n",
      "| Split 4 | 0.9467071057192374 |\n",
      "| Split 5 | 0.9506065857885615 |\n",
      "| Average | 0.9505199306759099 |\n",
      "+---------+--------------------+\n",
      "Kernal EDice\n",
      "time 4:02:49.223490\n",
      "measure ROC\n",
      "+---------+--------------------+\n",
      "|    k    |      DataSet1      |\n",
      "+---------+--------------------+\n",
      "| Split 1 | 0.9164230093415673 |\n",
      "| Split 2 | 0.8986345392845673 |\n",
      "| Split 3 | 0.895205912369791  |\n",
      "| Split 4 | 0.8929314586591304 |\n",
      "| Split 5 | 0.8941496549818995 |\n",
      "| Average | 0.8994689149273911 |\n",
      "+---------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# importing all the required modules\n",
    "\n",
    "from importlib import reload\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import os\n",
    "import errno\n",
    "import string\n",
    "from nltk.corpus import reuters\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.text import TextCollection\n",
    "import collections\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from prettytable import PrettyTable\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import recall_score,precision_score,average_precision_score,f1_score,accuracy_score,roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.cluster import homogeneity_score,completeness_score\n",
    "import statistics\n",
    "import math\n",
    "import sklearn.metrics \n",
    "from sklearn.model_selection import KFold\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "import statistics\n",
    "from scipy.stats import pearsonr,entropy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def loadReutersData(documents,labels):\n",
    "    categories_list=['acq','crude','earn','grain','interest','money-fx','ship','trade']\n",
    "    docCount=0\n",
    "    for i in range(0,len(categories_list)):\n",
    "        category_docs = reuters.fileids(categories_list[i])\n",
    "        print (categories_list[i])\n",
    "        for document_id in reuters.fileids(categories_list[i]):\n",
    "            if(len(reuters.categories(document_id))==1):\n",
    "                content=str(reuters.raw(document_id))\n",
    "                soup = BeautifulSoup(content)\n",
    "                content=soup.get_text()\n",
    "                documents.append(content)\n",
    "                docCount+=1\n",
    "                labels.append(str(reuters.categories(document_id)))\n",
    "def loadWebKbData(path, documents,labels):\n",
    "    print(path)\n",
    "    for root, dirs, files in os.walk(path):  \n",
    "        for filename in files:\n",
    "            try:\n",
    "                #print root\n",
    "                name = os.path.join(root, filename)\n",
    "                #print name\n",
    "                end=len(name)-len(filename)\n",
    "                test=name[len(path)+1:end]\n",
    "                for i in range(0,len(test)):\n",
    "                    if test[i]=='\\\\':\n",
    "                        labels.append(test[0:i])\n",
    "                        break\n",
    "                f = open(name, \"rb\").read()\n",
    "                f=f.decode('ISO-8859-1', 'ignore')\n",
    "                content=str(f)\n",
    "                rawData.append(f)\n",
    "                soup = BeautifulSoup(content)\n",
    "                content=soup.get_text()\n",
    "                documents.append(content)  \n",
    "            except IOError as exc:\n",
    "                if exc.errno != errno.EISDIR:\n",
    "                       raise\n",
    "\n",
    "'''\n",
    "*****************************************Distnace measures*******************************************\n",
    "\n",
    "'''\n",
    "def Manhattan(doc1,doc2):\n",
    "    return -1*distance.cityblock(doc1,doc2)                       \n",
    "def Euclidean(a, b):#distance\n",
    "    return -1*distance.euclidean(a,b)\n",
    "def Cosine(a, b):#distance\n",
    "    return 1-distance.cosine(a,b)\n",
    "def Jaccard(a, b):#distance\n",
    "    return 1-distance.jaccard(a,b)\n",
    "def EnhancedJaccard(a, b):#distance\n",
    "    a=[0 if x==0 else 1 for x in a]\n",
    "    b=[0 if x==0 else 1 for x in b]\n",
    "    return 1-distance.jaccard(a,b)\n",
    "def KL(a, b):\n",
    "    return 1-entropy(a,b)\n",
    "def PCC(a, b):\n",
    "    pcc, col=pearsonr(a,b)\n",
    "    return abs(pcc)\n",
    "def extendedJaccard(a,b):\n",
    "    vector1=[0 if x==0 else 1 for x in a]\n",
    "    vector2=[0 if x==0 else 1 for x in b]\n",
    "    dot=np.dot(vector1,vector2)\n",
    "    sum1=np.sum(vector1)\n",
    "    sum2=np.sum(vector2)\n",
    "    denom=math.sqrt(sum1)+math.sqrt(sum2)-dot\n",
    "    if(denom!=0):\n",
    "        return  (float(dot)/(denom))\n",
    "    else:\n",
    "        return -1\n",
    "def bhatta(a,b):\n",
    "\n",
    "    length=len(a)\n",
    "    score = 0;\n",
    "    score=np.sum(np.sqrt( np.multiply(a,b) ))\n",
    "    distance=-1*np.log(score)\n",
    "    return 1-distance;        \n",
    "\n",
    "def JS(a, b):\n",
    "   # normalize\n",
    "    p = a/ np.sum(a)\n",
    "    q = b/ np.sum(b)\n",
    "    m = (p + q) / 2\n",
    "    return 1-(entropy(p, m) + entropy(q, m)) / 2\n",
    "\n",
    "def Pairwise(a,b):\n",
    "    percentage=1 #100 percent\n",
    "    K1=np.count_nonzero(a)\n",
    "    K2=np.count_nonzero(b)\n",
    "    k=percentage*min(K1,K2)\n",
    "   # setA=set((-a).argsort()[:k])\n",
    "   # setB=set((-b).argsort()[:k])\n",
    "    \n",
    "    setA=set(np.argpartition(a, len(a) - k)[-k:])\n",
    "    setB=set(np.argpartition(b, len(b) - k)[-k:])\n",
    "    union=setA.union(setB)\n",
    "    elementsA=[a[ind] for ind in union]\n",
    "    elementsB=[b[ind] for ind in union]\n",
    "    dist= distance.cosine(elementsA, elementsB)\n",
    "    return 1-dist\n",
    "\n",
    "def Dice(a, b):\n",
    "    a=[(a.astype(bool)).astype(int)]\n",
    "    b=[(b.astype(bool)).astype(int)]\n",
    "    return 1-distance.dice(a,b)\n",
    "def Extended_Dice(a, b):\n",
    "    a=[(a.astype(bool)).astype(int)]\n",
    "    b=[(b.astype(bool)).astype(int)]\n",
    "    sim=(2*np.dot(a,b))/(sum(a)**2 + sum(b)**2)\n",
    "    return sim\n",
    "def simIT(a,b,termOcc,docOcc):\n",
    "    p1= np.divide(a,termOcc) \n",
    "    p2= np.divide(b,termOcc)\n",
    "    minVal=np.minimum(p1,p2)\n",
    "    pi=np.log((np.array(docOcc)/totalDocs))\n",
    "    sIT=(2*np.sum(np.multiply(minVal,pi)))/((np.sum(np.multiply(p1,pi)))+(np.sum(np.multiply(p2,pi))))\n",
    "    return (sIT)\n",
    "def SP(doc1,doc2,N,allData):\n",
    "    \n",
    "    a=set(np.nonzero(doc1)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(doc2)[0])  #indices of non zero elements in doc2\n",
    "    nonzeroTerm=list(a.intersection(b))\n",
    "    minArr=np.minimum(doc1[nonzeroTerm],doc2[nonzeroTerm])\n",
    "    maxArr=np.maximum(doc1[nonzeroTerm],doc2[nonzeroTerm])\n",
    "    ln=len(nonzeroTerm)\n",
    "    docCount=np.zeros(ln)\n",
    "    for ti in range(0,ln):\n",
    "        tData=pd.Series(allData[:,ti])\n",
    "        docCount[ti]=count_values_in_range(tData, minArr[ti], maxArr[ti])\n",
    "    normFactor= len(a.union(b))\n",
    "    dCount=np.array(docCount[np.nonzero(docCount)[0]])\n",
    "    SP_val=0\n",
    "    if(normFactor!=0):\n",
    "        SP_val=sum(np.log(totalDocs/dCount))\n",
    "        SP_val = (SP_val/normFactor)\n",
    "    return SP_val\n",
    "\n",
    "def PDSM1(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(doc2)[0])  #indices of non zero elements in doc2\n",
    "    intersection=sum(np.minimum(doc1,doc2))\n",
    "    union=sum(np.maximum(doc1,doc2))\n",
    "    return(intersection/union)\n",
    "    return psdm\n",
    "def PDSM(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(doc2)[0])  #indices of non zero elements in doc2\n",
    "    intersection=sum(np.minimum(doc1,doc2))\n",
    "    union=sum(np.maximum(doc1,doc2))\n",
    "    PF=len(a.intersection(b))\n",
    "    M=len(doc1)\n",
    "    AF=len(set(range(0,M))-(a.union(b)))\n",
    "    psdm=(intersection/union)*((PF+1)/(M-AF+1))\n",
    "    return psdm\n",
    "\n",
    "def smtp(doc1,doc2,var):\n",
    "    lemda=1\n",
    "    a=set(np.nonzero(doc1)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(doc2)[0])  #indices of non zero elements in doc2\n",
    "    intersection=a.intersection(b) # indices where both docs have non zero elements\n",
    "    union=a.union(b) # indices where either doc has non zero elements\n",
    "    d1=np.array(list(a-intersection)) # doc1 !=0 and doc2=0\n",
    "    d2=np.array(list(b-intersection)) # doc1 =0 and doc2!=0\n",
    "    doc1=np.array(doc1)\n",
    "    doc2=np.array(doc2)\n",
    "    Nstar=0\n",
    "    intersection=np.array(list(intersection))\n",
    "    if (len(intersection)>0):\n",
    "        term1=np.exp(-1*np.square(( doc1[intersection]-doc2[intersection] )/var[intersection]))\n",
    "        Nstar=sum(0.5* (1+term1)) +lemda* -1 *(len(d1)+len(d2))\n",
    "    else:\n",
    "        Nstar=lemda* -1 *(len(d1)+len(d2))   \n",
    "    Nunion=len(intersection)+len(d1)+len(d2)\n",
    "    smtp=((Nstar/Nunion)+lemda)/(1+lemda)\n",
    "    return smtp\n",
    "def ISC(doc1,doc2): \n",
    "    dot=sum(np.sqrt(np.multiply(doc1,doc2)))\n",
    "    isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
    "    return isc\n",
    "\n",
    "def NSMT(doc1,doc2):\n",
    "    Dij=Nij=Di=Ni=Nj=Dj=0\n",
    "    a=set(np.nonzero(doc1)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(doc2)[0])  #indices of non zero elements in doc2\n",
    "    intersection=a.intersection(b)# indices where both docs have non zero elements\n",
    "    a=np.array(list(a-intersection)) \n",
    "    b=np.array(list(b-intersection))\n",
    "    intersection=np.array(list(intersection))\n",
    "    if len(intersection)>0:\n",
    "      #  prod=np.multiply(doc1[intersection],doc2[intersection])\n",
    "        Dij=np.dot(doc1[intersection],doc2[intersection])#sum(prod)\n",
    "    if len(a)>0:\n",
    "        Di=sum(doc1[a])\n",
    "    if len(b)>0:\n",
    "        Dj=sum(doc2[b])\n",
    "    Nij=len(intersection)\n",
    "    Ni=len(a)\n",
    "    Nj=len(b)\n",
    "    Nsmt=(Nij*Dij)/(Ni*Di+Dj*Nj)\n",
    "    return Nsmt\n",
    "def CSMB(doc1,doc2,alpha=0.5,Beta=0.5):\n",
    "    sim1=CSM_P1(doc1,doc2)\n",
    "    sim2=CSM_P2(doc1,doc2)\n",
    "    simValue=alpha*sim1+Beta*sim2\n",
    "    return simValue\n",
    "def CSMB_MinMax(doc1,doc2,alpha=0.5,Beta=0.5):\n",
    "    sim1=CSM_P1(doc1,doc2)\n",
    "    sim2=CSM_P2(doc1,doc2)\n",
    "    simValue=alpha*max(sim1,sim2)+Beta*min(sim1,sim2)\n",
    "    return simValue\n",
    "def BLAB_SM(doc1,doc2):\n",
    "    sim1=0\n",
    "    if dataset==\"reuters\":\n",
    "        sim1=CSMB_MinMax(doc1,doc2,alpha=0.7,Beta=0.3)#CSMB12\n",
    "    else:\n",
    "        sim1=CSMB(doc1,doc2,alpha=0.9,Beta=0.1)# CSMB10\n",
    "    return sim1\n",
    "\n",
    "def ZSM(doc1,doc2):\n",
    "    Dij=Nij=Di=Ni=Nj=Dj=0\n",
    "    a=set(np.nonzero(doc1)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(doc2)[0])  #indices of non zero elements in doc2\n",
    "    intersection=a.intersection(b)# indices where both docs have non zero elements\n",
    "    intersection=np.array(list(intersection))\n",
    "    if len(intersection)>0:\n",
    "      #  prod=np.multiply(doc1[intersection],doc2[intersection])\n",
    "        Dij=np.dot(doc1[intersection],doc2[intersection])#sum(prod)\n",
    "    Nij=len(intersection)\n",
    "    zsm=(Nij*Dij)/(np.sum(doc1)*np.sum(doc2))\n",
    "    return zsm\n",
    "def CSM_P1(doc1,doc2):\n",
    "    if ((not np.any(doc1)) or (not np.any( doc2))):   \n",
    "        simValue=0  \n",
    "    else:\n",
    "        simValue=0\n",
    "        N=len(doc1)#total_features;\n",
    "        a=set(np.nonzero(doc1)[0])  \n",
    "        b=set(np.nonzero(doc2)[0])   \n",
    "        Nab=len(a.intersection(b))\n",
    "        F=len(a.union(b))-Nab\n",
    "        sim1=(1-F/N)\n",
    "    return sim1\n",
    "def CSM_P2(doc1,doc2):\n",
    "    if ((not np.any(doc1)) or (not np.any( doc2))):   \n",
    "        simValue=0  \n",
    "    else:\n",
    "        simValue=0\n",
    "        a=set(np.nonzero(doc1)[0])  \n",
    "        b=set(np.nonzero(doc2)[0])   \n",
    "        Na=len(a)\n",
    "        Nb=len(b)\n",
    "        Nab=len(a.intersection(b))\n",
    "        sim2=(2*Nab)/(Na+Nb)\n",
    "    return sim2\n",
    "def DDSMa(doc1,doc2):\n",
    "    return 1-(np.sum(np.abs(doc1-doc2))/np.sum(doc1+doc2))\n",
    "def DDSMb(doc1,doc2):\n",
    "    return 1-(np.sum(np.square(doc1-doc2))/np.sum(np.square(doc1+doc2)))\n",
    "def DDSMc(doc1,doc2):\n",
    "    doc3=np.square(doc1)\n",
    "    doc4=np.square(doc2)\n",
    "    return 1-(np.sum(np.abs(doc3-doc4))/np.sum(np.square(doc1+doc2)))\n",
    "def EN1_DDSMa(doc1,doc2):\n",
    "    return DDSMa(doc1,doc2)*CSM_P1(doc1,doc2)\n",
    "def EN_DDSMb(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  \n",
    "    b=set(np.nonzero(doc2)[0])\n",
    "    SF=len(list(a.intersection(b)))\n",
    "    N=len(doc1)\n",
    "    return DDSMb(doc1,doc2)*(SF+1)/(N+1)\n",
    "def EN_DDSMc(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  \n",
    "    b=set(np.nonzero(doc2)[0])\n",
    "    SF=len(list(a.intersection(b)))\n",
    "    N=len(doc1)\n",
    "    return DDSMc(doc1,doc2)*(SF+1)/(N+1)\n",
    "def BASM(doc1,doc2):\n",
    "    return CSM_P2(doc1,doc2) \n",
    "\n",
    "def ESTB_SM(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  \n",
    "    b=set(np.nonzero(doc2)[0])\n",
    "    intersection=np.array(list(a.intersection(b)))\n",
    "    comp1=np.array(list(a-b))\n",
    "    comp2=np.array(list(b-a))\n",
    "    X,Y,D1,D2,sim=0,0,0,0,0                   \n",
    "    if len(intersection)>0:\n",
    "        X=np.sum(doc1[intersection])\n",
    "        Y=np.sum(doc2[intersection])\n",
    "        if len(comp1)>0:\n",
    "            D1=np.sum(doc1[comp1])\n",
    "        if len(comp2)>0:\n",
    "            D2=np.sum(doc2[comp2])\n",
    "        if len(intersection)>0:\n",
    "            sim=1/(1+((D1/X)+(D2/Y)))\n",
    "    return sim \n",
    "def ESTBSM_Sim2(doc1,doc2):\n",
    "    return ESTB_SM(doc1,doc2)*CSM_P2(doc1,doc2)\n",
    "def pdsm1_Sim2(doc1,doc2):\n",
    "    return PDSM1(doc1,doc2)*CSM_P2(doc1,doc2)\n",
    "def mstbSM_Sim2(doc1,doc2):\n",
    "    return MSTB_SM(doc1,doc2)*CSM_P2(doc1,doc2)\n",
    "\n",
    "def ESTB_V1(doc1,doc2):\n",
    "    return ESTB_SM(doc1,doc2)+CSM_P2(doc1,doc2)\n",
    "def ESTB_V2(doc1,doc2):\n",
    "    return ESTB_SM(doc1,doc2)*CSM_P2(doc1,doc2)*ANSM(doc1,doc2)\n",
    "def ESTB_V3(doc1,doc2):\n",
    "    return ((ESTB_SM(doc1,doc2)+ANSM(doc1,doc2))/2)*CSM_P2(doc1,doc2)\n",
    "\n",
    "def EMX1(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  \n",
    "    b=set(np.nonzero(doc2)[0])\n",
    "    intersection=a.intersection(b)\n",
    "    sim=0\n",
    "    if len(intersection)>0:\n",
    "        meanDiff=abs(np.mean(doc1)-np.mean(doc2))\n",
    "        sim=1/(1+math.exp(-1*meanDiff))\n",
    "    return sim\n",
    "def EMX2(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  \n",
    "    b=set(np.nonzero(doc2)[0])\n",
    "    intersection=a.intersection(b)\n",
    "    sim=0\n",
    "    if len(intersection)>0:\n",
    "        meanDiff=abs(np.mean(doc1)-np.mean(doc2))\n",
    "        stdDiff=abs(np.std(doc1)-np.std(doc2))\n",
    "        sim=1/(1+math.exp(-1*meanDiff*stdDiff))\n",
    "    return sim \n",
    "\n",
    "def EMX7(doc1,doc2):\n",
    "    sim=EMX1(doc1,doc2)\n",
    "    if sim>0:\n",
    "        sim=sim*ESTB_SM(doc1,doc2)*CSM_P2(doc1,doc2)\n",
    "    return sim \n",
    "def EMX13(doc1,doc2):\n",
    "    sim=EMX2(doc1,doc2)\n",
    "    if sim>0:\n",
    "        sim=sim*ESTB_SM(doc1,doc2)*CSM_P2(doc1,doc2)\n",
    "    return sim \n",
    "def PCC_Sim2(doc1,doc2):#distance\n",
    "    return PCC(doc1,doc2)*CSM_P2(doc1,doc2)\n",
    "def EISC(doc1,doc2): \n",
    "    dot=np.sum(np.sqrt(np.multiply(doc1,doc2)))\n",
    "    isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
    "    return isc\n",
    "def EISC_Sim2(doc1,doc2):\n",
    "    return EISC(doc1,doc2)*CSM_P2(doc1,doc2)\n",
    "def ANSM(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(doc2)[0])  #indices of non zero elements in doc2\n",
    "    com=np.array(list(a.intersection(b)))\n",
    "    sim=0\n",
    "    if len(com)>0:\n",
    "        sim=np.sum(np.sum(doc1[com]+doc2[com]))/(np.sum(doc1)+np.sum(doc2))\n",
    "        \n",
    "    return sim   \n",
    "def ANSM_V1(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(doc2)[0])  #indices of non zero elements in doc2\n",
    "    SP=len(a.intersection(b))\n",
    "    N=len(doc1)\n",
    "    return ANSM(doc1,doc2)*((SP+1)/(N+1))\n",
    "def ANSM_V2(doc1,doc2):\n",
    "    return ANSM(doc1,doc2)*CSM_P2(doc1,doc2)\n",
    "def ANSM_V3(doc1,doc2):\n",
    "    return (ANSM(doc1,doc2)+CSM_P2(doc1,doc2))/2\n",
    "def MSTB_SM(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  \n",
    "    b=set(np.nonzero(doc2)[0])\n",
    "    intersection=np.array(list(a.intersection(b)))\n",
    "    sim=0\n",
    "    if len(intersection)>0:\n",
    "        ints=np.sum(np.intersect1d(doc1,doc2))\n",
    "        D1=np.sum(np.unique(doc1))-ints\n",
    "        D2=np.sum(np.unique(doc2))-ints\n",
    "        X,Y=0,0  \n",
    "        X=np.sum(doc1[intersection])\n",
    "        Y=np.sum(doc2[intersection])\n",
    "        a=np.array(list(a))\n",
    "        b=np.array(list(b))\n",
    "        Z1,Z2=0,0\n",
    "        Z1=np.sum(doc1[a])\n",
    "        Z2=np.sum(doc2[b]) \n",
    "        sim=((X*Y)/(Z1*Z2))*(1-((D1*D2)/(Z1*Z2)))\n",
    "    return sim \n",
    "def STB_SM(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1)[0])  \n",
    "    b=set(np.nonzero(doc2)[0])\n",
    "    intersection=np.array(list(a.intersection(b)))\n",
    "    comp1=a-b\n",
    "    comp2=b-a   \n",
    "    comp1=np.array(list(comp1))\n",
    "    comp2=np.array(list(comp2))\n",
    "    a=np.array(list(a))\n",
    "    b=np.array(list(b))\n",
    "    X,Y,D1,D2,Z1,Z2,sim=0,0,0,0,0,0,0\n",
    "    if len(intersection)>0:\n",
    "        X=np.sum(doc1[intersection])\n",
    "        Y=np.sum(doc2[intersection])\n",
    "    if len(comp1)>0:\n",
    "        D1=np.sum(doc1[comp1])\n",
    "    if len(comp2)>0:\n",
    "        D2=np.sum(doc2[comp2])\n",
    "    if len(a)>0:\n",
    "        Z1=np.sum(doc1[a])\n",
    "    if len(b)>0:\n",
    "        Z2=np.sum(doc2[b]) \n",
    "    if Z1!=0 and Z2!=0:\n",
    "        sim=((X*Y)/(Z1*Z2))*(1-((D1*D2)/(Z1*Z2)))\n",
    "    return sim\n",
    "def DSM(doc1,doc2,var):\n",
    "    lemda=1\n",
    "    a=set(np.nonzero(doc1)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(doc2)[0])  #indices of non zero elements in doc2\n",
    "    intersection=a.intersection(b) # indices where both docs have non zero elements\n",
    "    union=a.union(b) # indices where either doc has non zero elements\n",
    "    doc1=np.array(doc1)\n",
    "    doc2=np.array(doc2)\n",
    "    Nstar=0\n",
    "    intersection=np.array(list(intersection))\n",
    "    if (len(intersection)>0):\n",
    "        term1=np.exp(-1*( doc1[intersection]-doc2[intersection] )/var[intersection])\n",
    "        Nstar=sum(0.5* (1+term1)) \n",
    "    else:\n",
    "        Nstar=lemda* -1 \n",
    "    Nunion=len(union)\n",
    "    dsm=((Nstar/Nunion)+lemda)/(1+lemda)\n",
    "    return dsm\n",
    "def TA(doc1,doc2):\n",
    "    a=np.sqrt(doc1.dot(doc1))\n",
    "    b=np.sqrt(doc2.dot(doc2))\n",
    "    dot=np.dot(doc1,doc2)**2\n",
    "    sim=0\n",
    "    if a<=b:\n",
    "        sim=dot/(a*(b**3))\n",
    "    else:\n",
    "        sim=dot/(b*(a**3))\n",
    "    return sim\n",
    "def HSD(doc1,doc2):\n",
    "    sum1=0\n",
    "    minimum=np.minimum(doc1, doc2)\n",
    "    maximum=np.maximum(doc1, doc2)\n",
    "    return 1-np.sum(1-((1+minimum)/(1+maximum)))\n",
    "\n",
    "######################Enhnaced Measures#################################################\n",
    "def EExtendedJaccard(a,b):\n",
    "    vector1=set(np.where(a!=0)[0])\n",
    "    vector2=set(np.where(b!=0)[0])\n",
    "    dot=len(vector1.intersection(vector2))\n",
    "    sum1=len(vector1)\n",
    "    sum2=len(vector2)\n",
    "    denom=math.sqrt(sum1)+math.sqrt(sum2)-dot\n",
    "    if(denom!=0):\n",
    "        return 1.0 - (float(dot)/(denom))\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def EPairwise(a,b):\n",
    "    percentage=1 #100 percent\n",
    "    K1=np.count_nonzero(a)\n",
    "    K2=np.count_nonzero(b)\n",
    "    k=percentage*min(K1,K2)\n",
    "    setA=set(np.argpartition(a, len(a)-1 - k)[-k:])\n",
    "    setB=set(np.argpartition(b, len(b) -1- k)[-k:])\n",
    "    union=np.array(list(setA.union(setB)))\n",
    "    elementsA=a[union]\n",
    "    elementsB=b[union]\n",
    "    return distance.cosine(elementsA, elementsB)\n",
    "\n",
    "def EJS(a, b):\n",
    "    p = a/ np.sum(a)\n",
    "    q = b/ np.sum(b)\n",
    "    m = np.add(p,q) / 2\n",
    "    return (entropy(p, m) + entropy(q, m)) / 2\n",
    "\n",
    "def EDice(doc1,doc2):\n",
    "    a=set(list(np.nonzero(doc1!=0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2!=0)[0]))  #indices of non zero elements in doc2\n",
    "    intr=len(a.intersection(b))\n",
    "    aComp=len(a-b)\n",
    "    bComp=len(b-a)\n",
    "    sim=0\n",
    "    if (intr+aComp+bComp)!=0:\n",
    "        sim=2*(intr/(2*(intr+aComp+bComp)))\n",
    "    return sim\n",
    "def EExtendedDice(doc1, doc2):\n",
    "    a=set(list(np.nonzero(doc1!=0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2!=0)[0]))  #indices of non zero elements in doc2\n",
    "    intr=len(a.intersection(b))\n",
    "    sim=(2*intr)/(len(a)**2 + len(b)**2)\n",
    "    return sim\n",
    "def EPDSM(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1!=0)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(doc2!=0)[0])  #indices of non zero elements in doc2\n",
    "    intersection=np.sum(np.minimum(doc1,doc2))\n",
    "    union=np.sum(np.maximum(doc1,doc2))\n",
    "    PF=len(a.intersection(b))\n",
    "    M=len(doc1)\n",
    "    AF=M-len(a.union(b))\n",
    "    psdm=(intersection/union)*((PF+1)/(M-AF+1))\n",
    "    return psdm\n",
    "def esmtp(doc1,doc2):\n",
    "    lemda=1\n",
    "    a=set(list(np.nonzero(doc1 != 0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2 != 0)[0]))  #indices of non zero elements in doc2\n",
    "    intersection=a.intersection(b) # indices where both docs have non zero elements\n",
    "    union=a.union(b) # indices where either doc has non zero elements\n",
    "    l1=len(a-intersection) # doc1 !=0 and doc2=0\n",
    "    l2=len(b-intersection) # doc1 =0 and doc2!=0\n",
    "    Nstar=0\n",
    "    intersection=np.array(list(intersection))\n",
    "    if (len(intersection)>0):\n",
    "        term1=np.exp(-1*np.square(( doc1[intersection]-doc2[intersection] )/var[intersection]))\n",
    "        Nstar=np.sum(0.5* (1+term1)) +lemda* -1 *(l1+l2)\n",
    "    else:\n",
    "        Nstar=lemda* -1 *(l1+l2)   \n",
    "    Nunion=len(intersection)+l1+l2\n",
    "    smtp=0\n",
    "    if Nunion!=0:\n",
    "        smtp=((Nstar/Nunion)+lemda)/(1+lemda)\n",
    "    return smtp\n",
    "def ENSMT(doc1,doc2):\n",
    "    Dij=Nij=Di=Ni=Nj=Dj=0\n",
    "    a=set(list(np.nonzero(doc1 != 0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2 != 0)[0]))  #indices of non zero elements in doc2\n",
    "    intersection=a.intersection(b)# indices where both docs have non zero elements\n",
    "    a=np.array(list(a-intersection)) \n",
    "    b=np.array(list(b-intersection))\n",
    "    intersection=np.array(list(intersection))\n",
    "    if len(intersection)>0:\n",
    "      #  prod=np.multiply(doc1[intersection],doc2[intersection])\n",
    "        Dij=np.dot(doc1[intersection],doc2[intersection])#sum(prod)\n",
    "    if len(a)>0:\n",
    "        Di=np.sum(doc1[a])\n",
    "    if len(b)>0:\n",
    "        Dj=np.sum(doc2[b])\n",
    "    Nij=len(intersection)\n",
    "    Ni=len(a)\n",
    "    Nj=len(b)\n",
    "    Nsmt=0\n",
    "    if (Ni*Di+Dj*Nj)!=0:\n",
    "        Nsmt=(Nij*Dij)/(Ni*Di+Dj*Nj)\n",
    "    return Nsmt\n",
    "\n",
    "def EBLAB_SM(doc1,doc2):\n",
    "    return 0.5*(ECSM_P1(doc1,doc2)+ECSM_P2(doc1,doc2))\n",
    "    '''\n",
    "    if dataset==\"reuters\":\n",
    "        sim1=ECSMB_MinMax(doc1,doc2,alpha=0.7,Beta=0.3)#CSMB12\n",
    "    else:\n",
    "        sim1=ECSMB(doc1,doc2,alpha=0.9,Beta=0.1)# CSMB10\n",
    "    \n",
    "    return sim1\n",
    "    '''\n",
    "def EESTB_SM(doc1,doc2):\n",
    "    a=set(list(np.nonzero(doc1 != 0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2 != 0)[0]))  #indices of non zero elements in doc2\n",
    "    intersection= np.array(list(a.intersection(b)))\n",
    "    comp1=np.array(list(a-b))\n",
    "    comp2=np.array(list(b-a))\n",
    "    D1=D2=sim=0                   \n",
    "    if len(intersection)>0:\n",
    "        X=np.sum(doc1[intersection])\n",
    "        Y=np.sum(doc2[intersection])\n",
    "        if len(comp1)>0:\n",
    "            D1=np.sum(doc1[comp1])\n",
    "        if len(comp2)>0:\n",
    "            D2=np.sum(doc2[comp2])\n",
    "        sim=1/(1+((D1/X)+(D2/Y)))\n",
    "    return sim\n",
    "\n",
    "\n",
    "def ECSM_P2(doc1,doc2):\n",
    "    a=set(list(np.nonzero(doc1 != 0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2 != 0)[0]))  #indices of non zero elements in doc2NSMT\n",
    "    sim=0\n",
    "    if (len(a)+len(b))!=0:\n",
    "        sim=(2*len(a.intersection(b)))/(len(a)+len(b))\n",
    "    return sim\n",
    "\n",
    "\n",
    "\n",
    "def EEMX1(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1!=0)[0])  \n",
    "    b=set(np.nonzero(doc2!=0)[0])\n",
    "    intersection=a.intersection(b)\n",
    "    intersection=a.intersection(b)\n",
    "    sim=0\n",
    "    if len(intersection)>0:\n",
    "        meanDiff=abs(np.mean(doc1)-np.mean(doc2))\n",
    "        sim=1/(1+math.exp(-1*meanDiff))\n",
    "    return sim\n",
    "\n",
    "\n",
    "def EEMX2(doc1,doc2):\n",
    "    a=set(np.nonzero(doc1!=0)[0])  \n",
    "    b=set(np.nonzero(doc2!=0)[0])\n",
    "    intersection=a.intersection(b)\n",
    "    sim=0\n",
    "    if len(intersection)>0:\n",
    "        meanDiff=abs(np.mean(doc1)-np.mean(doc2))\n",
    "        stdDiff=abs(np.std(doc1)-np.std(doc2))\n",
    "        sim=1/(1+math.exp(-1*meanDiff*stdDiff))\n",
    "    return sim \n",
    "\n",
    "def EEMX7(doc1,doc2):\n",
    "    sim=EEMX1(doc1,doc2)\n",
    "    if sim>0:\n",
    "        sim=sim*(1-EESTB_SM(doc1,doc2))*ECSM_P2(doc1,doc2)\n",
    "    return sim\n",
    "def EEMX13(doc1,doc2):\n",
    "    sim=EEMX2(doc1,doc2)\n",
    "    if sim>0:\n",
    "        sim=sim*(1-EESTB_SM(doc1,doc2))*ECSM_P2(doc1,doc2)\n",
    "    return sim \n",
    "\n",
    "def EMSTB_SM(doc1,doc2):\n",
    "    a=np.nonzero(doc1 != 0)[0]  #indices of non zero elements in doc1\n",
    "    b=np.nonzero(doc2 != 0)[0]  #indices of non zero elements in doc2\n",
    "    newDoc1=doc1[a]\n",
    "    newDoc2=doc2[b]\n",
    "    intersection=np.intersect1d(a,b)\n",
    "    sim=0\n",
    "    if len(intersection)>0:\n",
    "        ints=np.sum(np.intersect1d(doc1,doc2))\n",
    "        D1=np.sum(np.unique(newDoc1))-ints\n",
    "        D2=np.sum(np.unique(newDoc2))-ints \n",
    "        X=np.sum(doc1[intersection])\n",
    "        Y=np.sum(doc2[intersection])\n",
    "        Z1=np.sum(newDoc1)\n",
    "        Z2=np.sum(newDoc2) \n",
    "        sim=((X*Y)/(Z1*Z2))*(1-((D1*D2)/(Z1*Z2)))\n",
    "    return sim \n",
    "\n",
    "def ECSM_P1(doc1,doc2):\n",
    "    simValue=0\n",
    "    N=len(doc1)#total_features;\n",
    "    a=set(list(np.nonzero(doc1 != 0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2 != 0)[0]))  #indices of non zero elements in doc2NSMT\n",
    "    Nab=len(a.intersection(b))\n",
    "    F=len(a.union(b))-Nab\n",
    "    simValue=(1-F/N)\n",
    "    return simValue\n",
    "def DDSMa(doc1,doc2):\n",
    "    return 1-(np.sum(np.abs(doc1-doc2))/np.sum(doc1+doc2))\n",
    "def DDSMb(doc1,doc2):\n",
    "    return 1-(np.sum(np.square(doc1-doc2))/np.sum(np.square(doc1+doc2)))\n",
    "def DDSMc(doc1,doc2):\n",
    "    doc3=np.square(doc1)\n",
    "    doc4=np.square(doc2)\n",
    "    return 1-(np.sum(np.abs(doc3-doc4))/np.sum(np.square(doc1+doc2)))\n",
    "\n",
    "def EEN1_DDSMa(doc1,doc2):\n",
    "    return DDSMa(doc1,doc2)*ECSM_P1(doc1,doc2)\n",
    "def EEN_DDSMb(doc1,doc2):\n",
    "    a=set(list(np.nonzero(doc1 != 0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2 != 0)[0]))  #indices of non zero elements in doc2\n",
    "    SF=len(a.intersection(b))\n",
    "    N=len(doc1)\n",
    "    return DDSMb(doc1,doc2)*(SF+1)/(N+1)\n",
    "def EEN_DDSMc(doc1,doc2):\n",
    "    a=set(list(np.nonzero(doc1 != 0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2 != 0)[0]))  #indices of non zero elements in doc2\n",
    "    SF=len(a.intersection(b))\n",
    "    sim=1-(DDSMc(doc1,doc2)*(SF+1)/(len(doc1)+1))\n",
    "    return sim\n",
    "\n",
    "\n",
    "def EANSM(doc1,doc2):\n",
    "    a=np.nonzero(doc1 != 0)[0] #indices of non zero elements in doc1\n",
    "    b=np.nonzero(doc2 != 0)[0]  #indices of non zero elements in doc2\n",
    "    com=np.intersect1d(a,b)\n",
    "    sim=0\n",
    "    if len(com)>0:\n",
    "        sim=np.sum(doc1[com]+doc2[com])/np.sum(doc1+doc2)\n",
    "    return sim  \n",
    "def EANSM_V3(doc1,doc2):\n",
    "    return (EANSM(doc1,doc2)+ECSM_P2(doc1,doc2))/2\n",
    "\n",
    "def EZSM(doc1,doc2):\n",
    "    Dij=Nij=Di=Ni=Nj=Dj=0\n",
    "    a=np.nonzero(doc1 != 0)[0]  #indices of non zero elements in doc1\n",
    "    b=np.nonzero(doc2 != 0)[0]  #indices of non zero elements in doc2\n",
    "   # sum1=np.sum(doc1[a])\n",
    "    #sum2=np.sum(doc2[b])\n",
    "    a=set(list(a))\n",
    "    b=set(list(b)) \n",
    "    intersection=a.intersection(b)# indices where both docs have non zero elements\n",
    "    intersection=np.array(list(intersection))\n",
    "    Nij=len(intersection)\n",
    "    if Nij>0:\n",
    "        Dij=np.dot(doc1[intersection],doc2[intersection])#sum(prod)\n",
    "    zsm=(Nij*Dij)/(np.sum(doc1)*np.sum(doc2))\n",
    "    return zsm\n",
    "def EEnhancedJaccard(a, b):#similarity\n",
    "    a=set(np.nonzero(a!=0)[0])  #indices of non zero elements in doc1\n",
    "    b=set(np.nonzero(b!=0)[0])  #indices of non zero elements in doc2\n",
    "    union=a.union(b)\n",
    "    intersection=a.intersection(b)\n",
    "    sim=0\n",
    "    if len(union)!=0:\n",
    "        sim=len(intersection)/len(union)#similarity\n",
    "    return sim\n",
    "def esimIT(a,b):\n",
    "    p1= np.divide(a,termOcc) \n",
    "    p2= np.divide(b,termOcc)\n",
    "    minVal=np.minimum(p1,p2)\n",
    "    pi=np.log((np.array(docOcc)/totalDocs))\n",
    "    sIT=(2*np.sum(np.multiply(minVal,pi)))/((np.sum(np.multiply(p1,pi)))+(np.sum(np.multiply(p2,pi))))\n",
    "    return sIT\n",
    "def STB_SM_new(doc1,doc2):\n",
    "    a=set(list(np.nonzero(doc1 != 0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2 != 0)[0]))  #indices of non zero elements in doc2\n",
    "    intersection=np.array(list(a.intersection(b)))\n",
    "    comp1=a-b\n",
    "    comp2=b-a   \n",
    "    comp1=np.array(list(comp1))\n",
    "    comp2=np.array(list(comp2))\n",
    "    X,Y,D1,D2,Z1,Z2,sim=0,0,0,0,0,0,0\n",
    "    if len(intersection)>0:\n",
    "        X=np.sum(doc1[intersection])\n",
    "        Y=np.sum(doc2[intersection])\n",
    "    if len(comp1)>0:\n",
    "        D1=np.sum(doc1[comp1])\n",
    "    if len(comp2)>0:\n",
    "        D2=np.sum(doc2[comp2])\n",
    "    Z1=np.sum(doc1)\n",
    "    Z2=np.sum(doc2) \n",
    "    if Z1!=0 and Z2!=0:\n",
    "        sim=((X*Y)/(Z1*Z2))*(1-((D1*D2)/(Z1*Z2)))\n",
    "    return sim\n",
    "def EBASM(doc1,doc2):\n",
    "    return ECSM_P2(doc1,doc2) \n",
    "def EISC(doc1,doc2): \n",
    "    dot=np.sum(np.sqrt(np.multiply(doc1,doc2)))\n",
    "    isc=dot /( math.sqrt(np.linalg.norm(doc1, ord=1))*math.sqrt(np.linalg.norm(doc2, ord=1)))\n",
    "    return isc\n",
    "\n",
    "#SP equations\n",
    "def ESP(doc1,doc2):\n",
    "    a=set(list(np.nonzero(doc1!=0)[0]))  #indices of non zero elements in doc1\n",
    "    b=set(list(np.nonzero(doc2!=0)[0]))  #indices of non zero elements in doc2\n",
    "    nonzeroTerm=list(a.intersection(b))  #indices where both docs have non zero elemments\n",
    "    d1=doc1[nonzeroTerm] #doc1 values of doc1 intersection  doc2\n",
    "    d2=doc2[nonzeroTerm] #doc2 values of doc1 intersection  doc2\n",
    "    minArr=np.minimum(d1,d2) #minimum values  of d1 and d2\n",
    "    maxArr=np.maximum(d1,d2) #maximum values  of d1 and d2\n",
    "    ln=len(nonzeroTerm)\n",
    "    docCount=np.zeros(ln)\n",
    "    aData=allData[:,nonzeroTerm] #extracting data with only feature indices of doc1 intersection  doc2\n",
    "    for ti in range(0,ln):\n",
    "        docCount[ti]=count_values_in_range(aData[:,ti], minArr[ti], maxArr[ti]) #count values which lies between min and max\n",
    "    #norm factor which is ist part of equation 6 in paper\n",
    "    normFactor= len(a.union(b))#norm factor equal to length of doc1 union doc2\n",
    "    dCount=np.array(docCount[np.nonzero(docCount!=0)[0]])# get all non zero values between min and max\n",
    "    SP_val=0\n",
    "    if(normFactor!=0):\n",
    "        SP_val=np.sum(np.log(totalDocs/dCount)) #2nd part in equation 6 in paper\n",
    "        SP_val = (SP_val/normFactor) #equation 6 in paper\n",
    "    return SP_val\n",
    "def ENSMT_BASM(doc1,doc2):\n",
    "    return ENSMT(doc1,doc2)*BASM(doc1,doc2)\n",
    "    \n",
    "def PCC_BASM(doc1,doc2):\n",
    "    return PCC(doc1,doc2)*BASM(doc1,doc2)\n",
    "  \n",
    "'''\n",
    "***************************************** End of Distnace measures*******************************************\n",
    "\n",
    "'''\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "def tokenize1(documents):\n",
    "    tokens=[]\n",
    "    content= documents\n",
    "    tokens=(word_tokenize(content))\n",
    "    tokens= [token.lower() for token in tokens ]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    tokens= [token for token in tokens if token.isalpha()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens= [token for token in tokens if len(token)>3 ]\n",
    "    return tokens\n",
    "\n",
    "def display_scores(vectorizer, tfidf_result):\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "def sorted_tfs(tfs,n):\n",
    "    doc,terms=tfs.shape\n",
    "    ind=(np.argsort(-(np.asarray(tfs.sum(axis=0)).ravel())))\n",
    "    scores=np.zeros((doc,n))\n",
    "    for i in range(0,len(documents)):\n",
    "        for j in range(0,n):\n",
    "            if(tfs[i,ind[j]]!=0):\n",
    "                scores[i,j]=tfs[i,ind[j]]\n",
    "    return scores\n",
    "def count_values_in_range(series, range_min, range_max):\n",
    "\n",
    "    # \"between\" returns a boolean Series equivalent to left <= series <= right.\n",
    "    # NA values will be treated as False.\n",
    "    return series.between(left=range_min, right=range_max).sum()\n",
    "\n",
    "def dislay_tfidf(vectorizer,tfidf_result):\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(tfidf_result)\n",
    "    \n",
    "def display_scores(vectorizer, tfidf_result):\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores\n",
    "def groupData(labels,categories):\n",
    "\n",
    "    print (categories)\n",
    "    groups=[]\n",
    "    for i in range(0,len(categories)):\n",
    "        groups.append([0,0,0])\n",
    "    totalDocs=len(labels)\n",
    "    print (totalDocs)\n",
    "    for i in range(0,totalDocs):\n",
    "        tmp=(categories.index(labels[i]))\n",
    "        groups[tmp].append(i)\n",
    "    for i in range(0,len(categories)):\n",
    "        del (groups[i])[0:3]\n",
    "    return groups\n",
    "\n",
    "def classify(kernal='linear',termOccurance=None, docOccurance=None):\n",
    "   \n",
    "    kernals=['linear','poly', 'rbf', 'sigmoid']\n",
    "    if kernal in kernals:\n",
    "        classifier = svm.SVC(kernel=kernal).fit(train_data, train_labels)\n",
    "        y_pred = classifier.predict(test_data)\n",
    "    else:\n",
    "        classifier= svm.SVC(kernel=\"precomputed\")\n",
    "       \n",
    "        classifier = classifier.fit(GramMatrix(train_data,train_data,kernal), train_labels)\n",
    "        y_pred = classifier.predict( GramMatrix(test_data,train_data,kernal))\n",
    "    \n",
    "   \n",
    "    time2= time.time()\n",
    "    \n",
    "    for i in range(0,1):\n",
    "        #Accuracy\n",
    "    \n",
    "        accuracy=accuracy_score(test_labels, y_pred)\n",
    "        \n",
    "        #Macro Precision\n",
    "        \n",
    "        macroP=precision_score(test_labels, y_pred,average='macro')\n",
    "        \n",
    "        #Micro Precision\n",
    "        \n",
    "        microP=precision_score(test_labels, y_pred,average='micro')\n",
    "        \n",
    "        #Macro Recall\n",
    "        \n",
    "        macroR=recall_score(test_labels, y_pred,average='macro')\n",
    "        \n",
    "        #Micro Recall\n",
    "        \n",
    "        microR=recall_score(test_labels, y_pred,average='micro')\n",
    "        \n",
    "        \n",
    "        fMeasure=f1_score(test_labels, y_pred,average='macro')\n",
    "       # PrintDetails(metric=metric,time=str(timedelta(seconds=(time2-time1))),measure=\"F Measure\",Arr=fMeasure)\n",
    "        \n",
    "        m_fMeasure=f1_score(test_labels, y_pred,average='micro')\n",
    "       # PrintDetails(metric=metric,time=str(timedelta(seconds=(time2-time1))),measure=\"F Measure\",Arr=fMeasure)\n",
    "        \n",
    "        #PrintDetails(metric=metric,time=str(timedelta(seconds=(time2-time1))),measure=\"g Measure\",Arr=gMeasure)\n",
    "      \n",
    "        test = label_binarize(test_labels, classes=categories)\n",
    "        pred = label_binarize( y_pred, classes=categories)\n",
    "        \n",
    "        #roc\n",
    "        roc=roc_auc_score(test, pred)\n",
    "    #Accuracy\n",
    "    accuracy_avg.append(accuracy)\n",
    "    #Precision\n",
    "    macroP_avg.append(macroP)\n",
    "    microP_avg.append(microP)\n",
    "    #recall\n",
    "    macroR_avg.append(macroR)\n",
    "    microR_avg.append(microR)\n",
    "    #F measure\n",
    "    macroFMeasure_avg.append(fMeasure)\n",
    "    microFMeasure_avg.append( m_fMeasure)\n",
    "    #Roc\n",
    "    roc_avg.append(roc)\n",
    "    \n",
    "        \n",
    "def PrintDetails(metric=\"smtp\",time=None,measure= \"Accuracy\",Arr=None):\n",
    "    x = PrettyTable()\n",
    "    kList=[1,3,5,9,15,30,45,70,90,120]\n",
    "    x.field_names = [\"k\",\"DataSet1\"]\n",
    "    print(\"Kernal\",metric)\n",
    "    print(\"time\",time)\n",
    "    print(\"measure\",measure)\n",
    "    tables.write(\"Kernal\\t\"+str(metric)+\"\\n\")\n",
    "    tables.write(\"time\\t\"+str(time)+\"\\n\")\n",
    "    tables.write(\"measure\\t\"+str(measure)+\"\\n\")\n",
    "    average=0\n",
    "    for i in range(0,len(Arr)):\n",
    "        sp=\"Split \"+str(i+1)\n",
    "        x.add_row([sp,Arr[i]]) \n",
    "        average+=Arr[i]\n",
    "    x.add_row([\"Average\",average/len(Arr)])   \n",
    "    tables.write(str(x))\n",
    "    print (x)\n",
    "def GramMatrix(X1, X2,metric):\n",
    "    \"\"\"(Pre)calculates Gram Matrix K\"\"\"\n",
    "    func=globals()[metric]\n",
    "    gram_matrix= pairwise_distances(X1, X2,func,n_jobs=-1)\n",
    "    maxval=np.nanmax(gram_matrix[gram_matrix != np.inf])\n",
    "    gram_matrix[np.isnan(gram_matrix)]=maxval\n",
    "    gram_matrix[np.inf==gram_matrix]=maxval\n",
    "    gram_matrix[-np.inf==gram_matrix]=maxval\n",
    "    return gram_matrix\n",
    "dataset='Reuters'\n",
    "count=0\n",
    "documents=[]\n",
    "labels=[]\n",
    "rawData=[]\n",
    "token_dict = dict()\n",
    "if dataset=='webkb':\n",
    "    path = \"webkb-data.gtar\\\\webkb\"\n",
    "    loadWebKbData(path=path,documents=documents,labels=labels)\n",
    "else:\n",
    "    loadReutersData(documents=documents,labels=labels)\n",
    "print(len(documents))\n",
    "categories=list(set(labels))\n",
    "totalDocs= len(documents)\n",
    "print (totalDocs)\n",
    "stopwords = stopwords.words('english')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "vocabulary = set()\n",
    "term_docs=[]\n",
    "terms=[]\n",
    "totalDocs= len(documents)\n",
    "print (len(documents))\n",
    "print (\"vectorize\")\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize1)\n",
    "print (\"fit transform\")\n",
    "tfs = tfidf.fit_transform(documents)\n",
    "kernals=[\"esmtp\",\"EISC\",\"EDice\"]\n",
    "arr=tfs.todense()\n",
    "k_splits=5\n",
    "kf = StratifiedKFold(n_splits=k_splits)\n",
    "labels=np.array(labels)\n",
    "var=0\n",
    "for kernal in kernals:\n",
    "    fname='table_'+dataset+'_SVM_'+kernal+'_Tfidf_5RE.txt'\n",
    "    tables = open(fname, 'w')\n",
    "    #tables.write(\"No. of features\\t\"+str(n)+\"\\n\")\n",
    "    time1= time.time()\n",
    "    print(\"###############\")\n",
    "    accuracy_avg=[]\n",
    "    macroP_avg=[]\n",
    "    microP_avg=[]\n",
    "    macroR_avg=[]\n",
    "    microR_avg=[]\n",
    "    macroFMeasure_avg=[]\n",
    "    microFMeasure_avg=[]\n",
    "    roc_avg=[]\n",
    "    split=0\n",
    "    if kernal==\"smtp\" or kernal==\"esmtp\" or kernal==\"DSM\":\n",
    "        var1=np.var(arr,axis=0).reshape(-1,1).ravel()\n",
    "        var=np.asarray(var1).ravel()\n",
    "    rand_state=[0,1,10,42,30]\n",
    "    for i in range(0,k_splits):\n",
    "        xTrain, xTest, yTrain, yTest = train_test_split(arr, labels, test_size = 0.3, random_state = rand_state[i],stratify=labels)\n",
    "        train_data=np.array(xTrain)\n",
    "        test_data=np.array(xTest)\n",
    "        train_labels=yTrain\n",
    "        test_labels=yTest\n",
    "        tables.write(\"\\n*************\")\n",
    "        tables.write(\" Split\\t\"+str(i+1))\n",
    "        tables.write(\" ***********\\n\")\n",
    "        classify(kernal=kernal)\n",
    "        #print (\"nTerms\",n)\n",
    "        \n",
    "    \n",
    "    time2=time.time()\n",
    "    #Accuracy\n",
    "    PrintDetails(metric=kernal,time=str(timedelta(seconds=(time2-time1))),measure=\"Accuracy\",Arr=accuracy_avg)\n",
    "    #Precision\n",
    "    PrintDetails(metric=kernal,time=str(timedelta(seconds=(time2-time1))),measure=\"Macro Precision\",Arr=macroP_avg)\n",
    "    PrintDetails(metric=kernal,time=str(timedelta(seconds=(time2-time1))),measure=\"Micro Precision\",Arr=microP_avg)\n",
    "    #Recall\n",
    "    PrintDetails(metric=kernal,time=str(timedelta(seconds=(time2-time1))),measure=\"Macro Recall\",Arr=macroR_avg)\n",
    "    PrintDetails(metric=kernal,time=str(timedelta(seconds=(time2-time1))),measure=\"Micro Recall\",Arr=microR_avg)\n",
    "    #f measure\n",
    "    PrintDetails(metric=kernal,time=str(timedelta(seconds=(time2-time1))),measure=\"F Measure\",Arr=macroFMeasure_avg)\n",
    "\n",
    "    #f measure\n",
    "    PrintDetails(metric=kernal,time=str(timedelta(seconds=(time2-time1))),measure=\"Micro F Measure\",Arr=microFMeasure_avg)\n",
    "\n",
    "    #roc\n",
    "    PrintDetails(metric=kernal,time=str(timedelta(seconds=(time2-time1))),measure=\"ROC\",Arr=roc_avg)\n",
    "    tables.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdsd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-374fd374bd8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msdsd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sdsd' is not defined"
     ]
    }
   ],
   "source": [
    "sdsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
